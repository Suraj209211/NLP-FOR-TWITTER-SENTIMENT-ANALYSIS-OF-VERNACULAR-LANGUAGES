{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'nltk' has no attribute 'words_tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     34\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mThis is a sentence.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 35\u001b[0m     classifier \u001b[39m=\u001b[39m train(text)\n\u001b[1;32m     36\u001b[0m     \u001b[39mprint\u001b[39m(classifier\u001b[39m.\u001b[39mclassify(\u001b[39m\"\u001b[39m\u001b[39mThis is a sentence.\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     37\u001b[0m     lstm \u001b[39m=\u001b[39m rnn_model(x, y)\n",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(text):\n\u001b[0;32m---> 19\u001b[0m     tokens \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mwords_tokenizer(text)\n\u001b[1;32m     21\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(tokens)\n\u001b[1;32m     24\u001b[0m     training_set \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'nltk' has no attribute 'words_tokenizer'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import nltk\n",
    "\n",
    "def rnn_model(x, y):\n",
    "    # Create an LSTM with 2 hidden layers of 128 units each.\n",
    "    lstm = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(10000, 128),\n",
    "        tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "        tf.keras.layers.LSTM(128),\n",
    "        tf.keras.layers.Dense(10000, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    lstm.compile(optimizer = 'adam', loss='categorial_crossentropy' , metrics=['accuracy'])\n",
    "    lstm.fit(x,y, epochs = 30)\n",
    "    \n",
    "    return lstm\n",
    "\n",
    "def train(text):\n",
    "    tokens = nltk.words_tokenizer(text)\n",
    "    \n",
    "    vocabulary = set(tokens)\n",
    "    \n",
    "    \n",
    "    training_set = []\n",
    "    for token in vocabulary:\n",
    "        training_set.append((token, 1))\n",
    "        \n",
    "    \n",
    "    classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "    return classifier\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = \"This is a sentence.\"\n",
    "    classifier = train(text)\n",
    "    print(classifier.classify(\"This is a sentence.\"))\n",
    "    lstm = rnn_model(x, y)\n",
    "    print(lstm.summary())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "\n",
    "def rnn_model(x, y):\n",
    "    # Create an LSTM with 2 hidden layers of 128 units each.\n",
    "    lstm = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(10000, 128),\n",
    "        tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "        tf.keras.layers.LSTM(128),\n",
    "        tf.keras.layers.Dense(10000, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Train the model.\n",
    "    lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    lstm.fit(x, y, epochs=10)\n",
    "\n",
    "    return lstm\n",
    "\n",
    "def train(text):\n",
    "    # Tokenize the text.\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Create a vocabulary.\n",
    "    vocabulary = set(tokens)\n",
    "\n",
    "    # Create a training set.\n",
    "    training_set = []\n",
    "    for token in vocabulary:\n",
    "        training_set.append({token: 1})\n",
    "\n",
    "    # Train a classifier.\n",
    "    classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "    return classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def train(text):\n",
    "    # Tokenize the text.\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Create a vocabulary.\n",
    "    vocabulary = set(tokens)\n",
    "\n",
    "    # Create a training set.\n",
    "    training_set = []\n",
    "    for token in vocabulary:\n",
    "        training_set.append((token, 1))\n",
    "\n",
    "    # Train a classifier.\n",
    "    classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "    return classifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "4/4 [==============================] - 3s 132ms/step - loss: 46053.5547 - accuracy: 0.0000e+00\n",
      "Epoch 2/40\n",
      "4/4 [==============================] - 1s 131ms/step - loss: 46056.7734 - accuracy: 0.0000e+00\n",
      "Epoch 3/40\n",
      "4/4 [==============================] - 1s 136ms/step - loss: 46077.3906 - accuracy: 0.0000e+00\n",
      "Epoch 4/40\n",
      "4/4 [==============================] - 1s 137ms/step - loss: 46092.4102 - accuracy: 0.0000e+00\n",
      "Epoch 5/40\n",
      "4/4 [==============================] - 1s 138ms/step - loss: 46104.2539 - accuracy: 0.0000e+00\n",
      "Epoch 6/40\n",
      "4/4 [==============================] - 1s 138ms/step - loss: 46111.0703 - accuracy: 0.0000e+00\n",
      "Epoch 7/40\n",
      "4/4 [==============================] - 1s 135ms/step - loss: 46114.2695 - accuracy: 0.0000e+00\n",
      "Epoch 8/40\n",
      "4/4 [==============================] - 1s 132ms/step - loss: 46115.4492 - accuracy: 0.0000e+00\n",
      "Epoch 9/40\n",
      "4/4 [==============================] - 1s 146ms/step - loss: 46116.0859 - accuracy: 0.0000e+00\n",
      "Epoch 10/40\n",
      "4/4 [==============================] - 1s 129ms/step - loss: 46116.6055 - accuracy: 0.0000e+00\n",
      "Epoch 11/40\n",
      "4/4 [==============================] - 1s 136ms/step - loss: 46117.4609 - accuracy: 0.0000e+00\n",
      "Epoch 12/40\n",
      "4/4 [==============================] - 1s 133ms/step - loss: 46118.0195 - accuracy: 0.0000e+00\n",
      "Epoch 13/40\n",
      "4/4 [==============================] - 1s 130ms/step - loss: 46118.7539 - accuracy: 0.0000e+00\n",
      "Epoch 14/40\n",
      "4/4 [==============================] - 1s 133ms/step - loss: 46119.7461 - accuracy: 0.0000e+00\n",
      "Epoch 15/40\n",
      "4/4 [==============================] - 1s 130ms/step - loss: 46120.6445 - accuracy: 0.0000e+00\n",
      "Epoch 16/40\n",
      "4/4 [==============================] - 1s 130ms/step - loss: 46121.9648 - accuracy: 0.0000e+00\n",
      "Epoch 17/40\n",
      "4/4 [==============================] - 1s 132ms/step - loss: 46123.0898 - accuracy: 0.0000e+00\n",
      "Epoch 18/40\n",
      "4/4 [==============================] - 1s 134ms/step - loss: 46124.8711 - accuracy: 0.0000e+00\n",
      "Epoch 19/40\n",
      "4/4 [==============================] - 1s 133ms/step - loss: 46126.7148 - accuracy: 0.0000e+00\n",
      "Epoch 20/40\n",
      "4/4 [==============================] - 1s 142ms/step - loss: 46128.5664 - accuracy: 0.0000e+00\n",
      "Epoch 21/40\n",
      "4/4 [==============================] - 1s 129ms/step - loss: 46130.3867 - accuracy: 0.0000e+00\n",
      "Epoch 22/40\n",
      "4/4 [==============================] - 1s 133ms/step - loss: 46132.3867 - accuracy: 0.0000e+00\n",
      "Epoch 23/40\n",
      "4/4 [==============================] - 1s 131ms/step - loss: 46134.4297 - accuracy: 0.0000e+00\n",
      "Epoch 24/40\n",
      "4/4 [==============================] - 1s 134ms/step - loss: 46135.4258 - accuracy: 0.0000e+00\n",
      "Epoch 25/40\n",
      "4/4 [==============================] - 1s 137ms/step - loss: 46136.9492 - accuracy: 0.0000e+00\n",
      "Epoch 26/40\n",
      "4/4 [==============================] - 1s 138ms/step - loss: 46137.8516 - accuracy: 0.0000e+00\n",
      "Epoch 27/40\n",
      "4/4 [==============================] - 1s 134ms/step - loss: 46138.5781 - accuracy: 0.0000e+00\n",
      "Epoch 28/40\n",
      "4/4 [==============================] - 1s 260ms/step - loss: 46139.0664 - accuracy: 0.0000e+00\n",
      "Epoch 29/40\n",
      "4/4 [==============================] - 1s 137ms/step - loss: 46140.3047 - accuracy: 0.0000e+00\n",
      "Epoch 30/40\n",
      "4/4 [==============================] - 1s 133ms/step - loss: 46141.1641 - accuracy: 0.0000e+00\n",
      "Epoch 31/40\n",
      "4/4 [==============================] - 1s 338ms/step - loss: 46142.5781 - accuracy: 0.0000e+00\n",
      "Epoch 32/40\n",
      "4/4 [==============================] - 1s 133ms/step - loss: 46144.2188 - accuracy: 0.0000e+00\n",
      "Epoch 33/40\n",
      "4/4 [==============================] - 1s 231ms/step - loss: 46146.1602 - accuracy: 0.0000e+00\n",
      "Epoch 34/40\n",
      "4/4 [==============================] - 1s 140ms/step - loss: 46148.1445 - accuracy: 0.0000e+00\n",
      "Epoch 35/40\n",
      "4/4 [==============================] - 1s 289ms/step - loss: 46149.7969 - accuracy: 0.0000e+00\n",
      "Epoch 36/40\n",
      "4/4 [==============================] - 1s 182ms/step - loss: 46151.1289 - accuracy: 0.0000e+00\n",
      "Epoch 37/40\n",
      "4/4 [==============================] - 1s 145ms/step - loss: 46152.4961 - accuracy: 0.0000e+00\n",
      "Epoch 38/40\n",
      "4/4 [==============================] - 1s 143ms/step - loss: 46154.1250 - accuracy: 0.0000e+00\n",
      "Epoch 39/40\n",
      "4/4 [==============================] - 1s 135ms/step - loss: 46155.7500 - accuracy: 0.0000e+00\n",
      "Epoch 40/40\n",
      "4/4 [==============================] - 1s 139ms/step - loss: 46158.6289 - accuracy: 0.0000e+00\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 128)         1280000   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, None, 128)         131584    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10000)             1290000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2833168 (10.81 MB)\n",
      "Trainable params: 2833168 (10.81 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def lstm_model(x, y):\n",
    "    # Create an LSTM with 2 hidden layers of 128 units each.\n",
    "    lstm = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(10000, 128),\n",
    "        tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "        tf.keras.layers.LSTM(128),\n",
    "        tf.keras.layers.Dense(10000, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Train the model.\n",
    "    lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    lstm.fit(x, y, epochs=40)\n",
    "\n",
    "    return lstm\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x = tf.random.uniform((100, 100), minval=0, maxval=1, dtype=tf.float32)\n",
    "    y = tf.random.uniform((100, 10000), minval=0, maxval=1, dtype=tf.float32)\n",
    "\n",
    "    lstm = lstm_model(x,y)\n",
    "    print(lstm.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
